{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker built-in Training and Deployment with LinearLearner\n",
    "\n",
    "In this notebook, we use Sagemaker's built-in machine learning model __LinearLearner__ to predict the __log_votes__ field of our review dataset.\n",
    "\n",
    "* Find more details on the Sagemaker's __LinearLearner__ here: https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __rating:__ Rating of the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/examples/NLP-REVIEW-DATA-REGRESSION.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_votes\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_votes\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"log_votes\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill-in the missing values for reviewText below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stop Word Removal and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the stop word removal and text cleaning processes below. NLTK library provides a list of common stop words. We will use the list, but remove some of the words from that list. It is because those words are actually useful to understand the sentiment in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "    \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-processing the reviewText field\")\n",
    "df[\"reviewText\"] = process_text(df[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scaling numerical fields:\n",
    "\n",
    "We will apply min-max scaling to our rating field so that they will be between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating\"] = (df[\"rating\"] - df[\"rating\"].min())/(df[\"rating\"].max()-df[\"rating\"].min())\n",
    "df[\"time\"] = (df[\"time\"] - df[\"time\"].min())/(df[\"time\"].max()-df[\"time\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the training dataset into training, validation and test\n",
    "\n",
    "Sklearn library has a useful function to split datasets. We will use the __train_test_split()__ function. In the example below, we get 80% for training, 10% for validation and 10% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input: \"reviewText\", \"rating\" and \"time\"\n",
    "# Target: \"log_votes\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[[\"reviewText\",\n",
    "                                                      \"rating\",\n",
    "                                                      \"time\"]],\n",
    "                                                  df[\"log_votes\"].tolist(),\n",
    "                                                  test_size=0.20,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val,\n",
    "                                                y_val,\n",
    "                                                test_size=0.50,\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Computing Bag of Words Features\n",
    "\n",
    "We are using binary features here. TF and TF-IDF are also other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(binary=True,     # using binary features\n",
    "                                   max_features=50  # vocabulary limit\n",
    "                                  )\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"reviewText\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"reviewText\"].tolist())\n",
    "# Only transform\n",
    "X_test_text_vectors = tfidf_vectorizer.transform(X_test[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our vocabulary below. The number next to the word is its index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training with Sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call the Sagemaker `LinearLearner()` below. \n",
    "* __Compute power:__ We will use `train_instance_count` and `train_instance_type` parameters. This example uses `ml.m4.xlarge` resource for training. We can change the instance type for our needs (For example GPUs for neural networks). \n",
    "* __Model type:__ `predictor_type` is set to __'regressor'__ as we have a regression problem. For classification, we can choose between `binary_classifier` and `multiclass_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "linear_regressor = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                           train_instance_count=1,\n",
    "                                           train_instance_type='ml.m4.xlarge',\n",
    "                                           predictor_type='regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the `record_set()` function of our binary_estimator to set the training, validation, test parts of the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's merge our training features\n",
    "train_features = np.column_stack((X_train_text_vectors.toarray(),\n",
    "                                  X_train[\"rating\"].values,\n",
    "                                  X_train[\"time\"].values)).astype(\"float32\")\n",
    "# Let's merge our val features\n",
    "val_features = np.column_stack((X_val_text_vectors.toarray(),\n",
    "                                X_val[\"rating\"].values,\n",
    "                                X_val[\"time\"].values)).astype(\"float32\")\n",
    "# Let's merge our test features\n",
    "test_features = np.column_stack((X_test_text_vectors.toarray(),\n",
    "                                 X_test[\"rating\"].values,\n",
    "                                 X_test[\"time\"].values)).astype(\"float32\")\n",
    "\n",
    "train_records = linear_regressor.record_set(train_features,\n",
    "                                            np.array(y_train).astype(\"float32\"),\n",
    "                                            channel='train')\n",
    "val_records = linear_regressor.record_set(val_features,\n",
    "                                          np.array(y_val).astype(\"float32\"),\n",
    "                                          channel='validation')\n",
    "test_records = linear_regressor.record_set(test_features,\n",
    "                                           np.array(y_test).astype(\"float32\"),\n",
    "                                           channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit()` function applies a distributed version of the Stochastic Gradient Descent (SGD) algorithm and we are sending the data to it. We disabled logs with `logs=False`. You can remove that parameter to see more details about the process. __This process takes about 3-4 minutes on a ml.m4.xlarge instance.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor.fit([train_records, val_records, test_records], logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation\n",
    "\n",
    "In this section, we will look at how our model performs with our test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Analytics\n",
    "We can use Sagemaker analytics to get some performance metrics of our choice. This doesn't require us to deploy our model. Since this is a regression problem, we can check the mean squared error and absolute loss (mean absolute error).\n",
    "\\begin{align}\n",
    "MSE=\\frac{1}{n}\\sum_{examples}{(y-p)^2}\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "MAE=\\frac{1}{n}\\sum_{examples}{|y-p|}\n",
    "\\tag{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(linear_regressor._current_job_name, \n",
    "                                         metric_names = ['test:mse', 'test:absolute_loss']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Deploying the model and using the endpoint\n",
    "\n",
    "In the last part of this exercise, we will deploy our model to another instance of our choice. This will allow us to use this model in production environment. Deployed endpoints can be used with other AWS Services such as Lambda and API Gateway. A nice walkthrough is available here: https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/ if you are interested.\n",
    "\n",
    "__9.1 Deploy the model:__\n",
    "\n",
    "Run the following cell to deploy the model. We can use different instance types such as: `ml.t2.medium`, `ml.c4.xlarge` etc. __This will take some time to complete (Approximately 7-8 minutes).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor_predictor = linear_regressor.deploy(initial_instance_count = 1,\n",
    "                                                     instance_type = 'ml.t2.medium',\n",
    "                                                     endpoint_name = 'LinearLearnerEndpoint'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9.2 Test the endpoint:__\n",
    "\n",
    "Let's use the deployed endpoint. We will send our test data and get predictions of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get test data in batch size of 25 and make predictions.\n",
    "prediction_batches = [linear_regressor_predictor.predict(batch) for batch in np.array_split(test_features, 25)]\n",
    "\n",
    "# Let's get a list of predictions\n",
    "print([pred.label['score'].float32_tensor.values[0] for pred in prediction_batches[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following to delete the endpoint after you are done using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session.delete_endpoint(linear_regressor_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
