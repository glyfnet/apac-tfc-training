{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model and Threshold Calibration\n",
    "\n",
    "In this notebook, we go over the Logistic Regression method to predict the __isPositive__ field of our final dataset, while also having a look at how probability threshold calibration can help improve classifier's performance.\n",
    "\n",
    "1. Reading the dataset\n",
    "2. Exploratory data analysis and missing value imputation\n",
    "3. Stop word removal and stemming\n",
    "4. Splitting the training dataset into training and validation\n",
    "5. Computing Bag of Words features\n",
    "6. Fitting LogisticRegression and checking model performance\n",
    "    * Find more details on the __LogisticRegression__ here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "7. Ideas for improvement: Probability threshold calibration (optional) \n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Rating of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the datasets\n",
    "\n",
    "We will use the __pandas__ library to read our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory data analysis and missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the target distribution for our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of missing values:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill-in a placeholder for the __reviewText__ missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stop word removal and stemming\n",
    "\n",
    "We will apply the text processing methods discussed in the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "    \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-processing training reviewText\")\n",
    "df[\"reviewText\"] = process_text(df[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting the training dataset into training and validation\n",
    "\n",
    "Sklearn library has a useful function to split datasets. We will use the __train_test_split()__ function. In the example below, we get 90% of the data for training and 10% is left for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[\"reviewText\"].tolist(), # Input\n",
    "                                                  df[\"isPositive\"].tolist(), # Target field\n",
    "                                                  test_size=0.10, # 10% val, 90% tranining\n",
    "                                                  shuffle=True) # Shuffle the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing Bag of Words Features\n",
    "\n",
    "We are using binary features here. TF and TF-IDF are other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(binary=True,\n",
    "                                   max_features=50 # Limit the vocabulary size\n",
    "                                  )\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fitting LogisticRegression and checking model performance\n",
    "\n",
    "Let's fit __LogisticRegression__ from Sklearn library, and check the performance on the validation dataset.\n",
    "\n",
    "Find more details on __LogisticRegression__ here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "\n",
    "# To improve the performance of LogisticRegression we can tune its parameters, for example:\n",
    "# * regularization type: penalty = {l1, l2, elasticnet}\n",
    "# * regularization strength: C = {smaller values specify stronger regularization} \n",
    "#    !!! LogisticRegression regularized cost function: C*Cost(w) + penalty(w), \n",
    "# where w is the weights vector !!!\n",
    "# * addressing class imbalance: \n",
    "# class_weight = {balanced or {class label:weight, class label:weight}, ...}\n",
    "lrClassifier = LogisticRegression(penalty = 'l2',\n",
    "                                  C = 0.1,\n",
    "                                  class_weight = 'balanced')\n",
    "lrClassifier.fit(X_train_text_vectors, y_train)\n",
    "lrClassifier_val_predictions = lrClassifier.predict(X_val_text_vectors)\n",
    "\n",
    "print(\"LogisticRegression on Validation: Accuracy Score: %f, F1-score: %f\" % \\\n",
    "      (accuracy_score(y_val, lrClassifier_val_predictions), f1_score(y_val, lrClassifier_val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ideas for improvement: Probability threshold calibration (optional)\n",
    "\n",
    "Besides tuning __LogisticRegression__ hyperparameter values, one other path to improve a classifier's performance is to dig deeper into how the classifier actually assigns class membership.\n",
    "\n",
    "**Binary predictions versus probability predictions.** We often use __classifier.predict()__ to examine classifier binary predictions, while in fact the outputs of most classifiers are real-valued, not binary. For most classifiers in sklearn, the method __classifier.predict_proba()__ returns class probabilities as a two-dimensional numpy array of shape (n_samples, n_classes) where the classes are lexicographically ordered. \n",
    "\n",
    "For our example, let's look at the first 5 predictions we made, in binary format and in real-valued probability format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrClassifier.predict(X_val_text_vectors)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrClassifier.predict_proba(X_val_text_vectors)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are the predicted probabilities used to decide class membership?** On each row of predict_proba output, the probabilities values sum to 1. There are two columns, one for each response class: column 0 - predicted probability that each observation is a member of class 0; column 1 - predicted probability that each observation is a member of class 1. From the predicted probabilities, choose the class with the highest probability.\n",
    "\n",
    "The key here is that a **threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions: class 0, if predicted probability is less than 0.5; class 1, if predicted probability is greater than 0.5.\n",
    "\n",
    "**Can we improve classifier performance by changing the classification threshold?** Let's **adjust** the classification threshold to influence the performance of the classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Threshold calibration to improve model accuracy\n",
    "\n",
    "We calculate the accuracy using different values for the classification threshold, and pick the threshold that resulted in the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the accuracy using different values for the classification threshold, \n",
    "# and pick the threshold that resulted in the highest accuracy.\n",
    "highest_accuracy = 0\n",
    "threshold_highest_accuracy = 0\n",
    "\n",
    "thresholds = np.arange(0,1,0.01)\n",
    "scores = []\n",
    "for t in thresholds:\n",
    "    # set threshold to 't' instead of 0.5\n",
    "    y_val_other = (lrClassifier.predict_proba(X_val_text_vectors)[:,1] >= t).astype(float)\n",
    "    score = accuracy_score(y_val, y_val_other)\n",
    "    scores.append(score)\n",
    "    if(score > highest_accuracy):\n",
    "        highest_accuracy = score\n",
    "        threshold_highest_accuracy = t\n",
    "print(\"Highest Accuracy on Validation:\", highest_accuracy, \\\n",
    "      \", Threshold for the highest Accuracy:\", threshold_highest_accuracy)   \n",
    "\n",
    "# Let's plot the accuracy versus different choices of thresholds\n",
    "plt.plot([0.5, 0.5], [np.min(scores), np.max(scores)], linestyle='--')\n",
    "plt.plot(thresholds, scores, marker='.')\n",
    "plt.title('Accuracy versus different choices of thresholds')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Threshold calibration to improve model F1 score\n",
    "\n",
    "Similarly, various choices of classification thresholds would affect the Precision and Recall metrics. Precision and Recall are usually trade offs of each other, so when you can improve both at the same time, your model's overall performance is undeniably improved. To choose a threshold that balances Precision and Recall, we can plot the Precision-Recall curve and pick the point with the highest F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Calculate the precision and recall using different values for the classification threshold\n",
    "val_predictions_probs = lrClassifier.predict_proba(X_val_text_vectors)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, val_predictions_probs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Precision and Recall values from the curve above, we calculate the F1 scores using:\n",
    "\n",
    "$$\\text{F1_score} = \\frac{2*(\\text{Precision} * \\text{Recall})}{(\\text{Precision} + \\text{Recall})}$$\n",
    "\n",
    "and pick the threshold that gives the highest F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the F1 score using different values for the classification threshold, \n",
    "# and pick the threshold that resulted in the highest F1 score.\n",
    "highest_f1 = 0\n",
    "threshold_highest_f1 = 0\n",
    "\n",
    "f1_scores = []\n",
    "for id, threhold in enumerate(thresholds):\n",
    "    f1_score = 2*precisions[id]*recalls[id]/(precisions[id]+recalls[id])\n",
    "    f1_scores.append(f1_score)\n",
    "    if(f1_score > highest_f1):\n",
    "        highest_f1 = f1_score\n",
    "        threshold_highest_f1 = threhold\n",
    "print(\"Highest F1 score on Validation:\", highest_f1, \\\n",
    "      \", Threshold for the highest F1 score:\", threshold_highest_f1)\n",
    "\n",
    "# Let's plot the F1 score versus different choices of thresholds\n",
    "plt.plot([0.5, 0.5], [np.min(f1_scores), np.max(f1_scores)], linestyle='--')\n",
    "plt.plot(thresholds, f1_scores, marker='.')\n",
    "plt.title('F1 Score versus different choices of thresholds')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
